# -*- coding: utf-8 -*-
"""IRIS : “SIREKAP dan OCR: Inovasi atau Ancaman bagi Integritas Pemilu?”.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qmJ5EohWvcpKFZB37hJd_8NvBD-vZ3GY

# Information of Dataset
"""

import pandas as pd

# Load the DataFrame
df = pd.read_csv("integrated_sirekap_new.csv")

df.shape

df

import pandas as pd

df_d = df
# ... (your code to populate df_d['created_at'])

df_d['created_at'] = pd.to_datetime(df_d['created_at'])

# Filter data excluding the specific date
#filtered_data = df_d[df_d['created_at'].dt.date != pd.to_datetime('2024-03-08')]

# Create Series and DataFrame (assuming same logic as before)
by_date = pd.Series(df_d['created_at'].dt.date).value_counts().sort_index()
by_date.index = pd.DatetimeIndex(by_date.index)
df_date = by_date.rename_axis('date').reset_index(name='counts')

# Display DataFrame
df_date

import plotly.express as px
import plotly.graph_objs as go
fig = go.Figure(data=go.Scatter(x=df_date['date'].astype(dtype=str),
                                y=df_date['counts'],
                                marker_color='black', text="counts"))
fig.update_layout({"title": 'Tweets about Sirekap from Februari 2024 to March 2024 Week by Week',
                   "xaxis": {"title":"Time"},
                   "yaxis": {"title":"Total tweets"},
                   "showlegend": False})
fig.show()

# Find the most retweeted, liked, and replied-to tweets
most_retweeted_tweet = df.nlargest(1, 'retweet_count', 'all')
most_liked_tweet = df.nlargest(1, 'favorite_count', 'all')
most_replied_tweet = df.nlargest(1, 'reply_count', 'all')

# Extract relevant information into DataFrames
most_retweeted_info = pd.DataFrame({
    'Username': most_retweeted_tweet['username'],
    'Full Text': most_retweeted_tweet['full_text'],
    'Created At': most_retweeted_tweet['created_at'],
    'Retweet Count': most_retweeted_tweet['retweet_count']
})

most_liked_info = pd.DataFrame({
    'Username': most_liked_tweet['username'],
    'Full Text': most_liked_tweet['full_text'],
    'Created At': most_liked_tweet['created_at'],
    'Favorite Count': most_liked_tweet['favorite_count'],
    'Quote Count': most_liked_tweet['quote_count']
})

most_replied_info = pd.DataFrame({
    'Username': most_replied_tweet['username'],
    'Full Text': most_replied_tweet['full_text'],
    'Created At': most_replied_tweet['created_at'],
    'Reply Count': most_replied_tweet['reply_count']
})

most_retweeted_info

import requests
from PIL import Image
import matplotlib.pyplot as plt
from io import BytesIO

# URL gambar di GitHub
image_url = "https://raw.githubusercontent.com/arknsa/image/main/top_1_replied.jpg"

# Mengambil gambar dari URL
response = requests.get(image_url)
img = Image.open(BytesIO(response.content))

# Menampilkan gambar
plt.figure(figsize=(8, 6))
plt.imshow(img)
plt.axis('off')  # Tidak menampilkan sumbu
plt.show()

most_liked_info

import requests
from PIL import Image
import matplotlib.pyplot as plt
from io import BytesIO

# URL gambar di GitHub
image_url = "https://raw.githubusercontent.com/arknsa/image/main/top_1_liked.jpg"

# Mengambil gambar dari URL
response = requests.get(image_url)
img = Image.open(BytesIO(response.content))

# Menampilkan gambar
plt.figure(figsize=(8, 6))
plt.imshow(img)
plt.axis('off')  # Tidak menampilkan sumbu
plt.show()

most_replied_info

import requests
from PIL import Image
import matplotlib.pyplot as plt
from io import BytesIO

# URL gambar di GitHub
image_url = "https://raw.githubusercontent.com/arknsa/image/main/top_1_replied.jpg"

# Mengambil gambar dari URL
response = requests.get(image_url)
img = Image.open(BytesIO(response.content))

# Menampilkan gambar
plt.figure(figsize=(8, 6))
plt.imshow(img)
plt.axis('off')  # Tidak menampilkan sumbu
plt.show()

"""## Before 18th February 2024"""

# Convert 'created_at' column to datetime format
df['created_at'] = pd.to_datetime(df['created_at'], format='%a %b %d %H:%M:%S %z %Y')

# Separate data before and after Feb 18
before_feb_18 = df[df['created_at'] < '2024-02-18 00:00:00+00:00']

before_feb_18.head()

# Find the top 3 most retweeted tweets before 18th
top_3_retweeted_before_18 = before_feb_18.nlargest(3, 'retweet_count')

# Find the top 3 most liked tweets before 18th
top_3_liked_before_18 = before_feb_18.nlargest(3, 'favorite_count')

# Find the top 3 most replied-to tweets before 18th
top_3_replied_before_18 = before_feb_18.nlargest(3, 'reply_count')

# Extract relevant information into DataFrames for each category
top_3_retweeted_before_18_info = pd.DataFrame({
    'Username': top_3_retweeted_before_18['username'],
    'Full Text': top_3_retweeted_before_18['full_text'],
    'Created At': top_3_retweeted_before_18['created_at'],
    'Retweet Count': top_3_retweeted_before_18['retweet_count']
})


top_3_liked_before_18_info = pd.DataFrame({
    'Username': top_3_liked_before_18['username'],
    'Full Text': top_3_liked_before_18['full_text'],
    'Created At': top_3_liked_before_18['created_at'],
    'Favorite Count': top_3_liked_before_18['favorite_count'],
    'Quote Count': top_3_liked_before_18['quote_count']
})


top_3_replied_before_18_info = pd.DataFrame({
    'Username': top_3_replied_before_18['username'],
    'Full Text': top_3_replied_before_18['full_text'],
    'Created At': top_3_replied_before_18['created_at'],
    'Reply Count': top_3_replied_before_18['reply_count']
})

top_3_retweeted_before_18_info

top_3_liked_before_18_info

top_3_replied_before_18_info

"""## On 18th February 2024

### On X
"""

on_feb_18 = df[(df['created_at'] >= '2024-02-18 00:00:00+00:00') & (df['created_at'] <= '2024-02-18 23:59:59+00:00')]

on_feb_18.head()

# Find the top 3 most retweeted tweets on 18th
top_3_retweeted_on_feb_18 = on_feb_18.nlargest(3, 'retweet_count')

# Find the top 3 most liked tweets on 18th
top_3_liked_on_feb_18 = on_feb_18.nlargest(3, 'favorite_count')

# Find the top 3 most replied-to tweets on 18th
top_3_replied_on_feb_18 = on_feb_18.nlargest(3, 'reply_count')

# Extract relevant information into DataFrames for each category
top_3_retweeted_on_feb_18_info = pd.DataFrame({
    'Username': top_3_retweeted_on_feb_18['username'],
    'Full Text': top_3_retweeted_on_feb_18['full_text'],
    'Created At': top_3_retweeted_on_feb_18['created_at'],
    'Retweet Count': top_3_retweeted_on_feb_18['retweet_count']
})

top_3_liked_on_feb_18_info = pd.DataFrame({
    'Username': top_3_liked_on_feb_18['username'],
    'Full Text': top_3_liked_on_feb_18['full_text'],
    'Created At': top_3_liked_on_feb_18['created_at'],
    'Favorite Count': top_3_liked_on_feb_18['favorite_count'],
    'Quote Count': top_3_liked_on_feb_18['quote_count']
})

top_3_replied_on_feb_18_info = pd.DataFrame({
    'Username': top_3_replied_on_feb_18['username'],
    'Full Text': top_3_replied_on_feb_18['full_text'],
    'Created At': top_3_replied_on_feb_18['created_at'],
    'Reply Count': top_3_replied_on_feb_18['reply_count']
})

top_3_retweeted_on_feb_18_info

top_3_liked_on_feb_18_info

top_3_replied_on_feb_18_info

"""### On Another Platform"""

import requests
from PIL import Image
import matplotlib.pyplot as plt
from io import BytesIO

# URL gambar di GitHub
image_url = "https://raw.githubusercontent.com/arknsa/image/main/on_18_1.jpg"

# Mengambil gambar dari URL
response = requests.get(image_url)
img = Image.open(BytesIO(response.content))

# Menampilkan gambar
plt.figure(figsize=(8, 6))
plt.imshow(img)
plt.axis('off')  # Tidak menampilkan sumbu
plt.show()

import requests
from PIL import Image
import matplotlib.pyplot as plt
from io import BytesIO

# URL gambar di GitHub
image_url = "https://raw.githubusercontent.com/arknsa/image/main/on_18_2.jpg"

# Mengambil gambar dari URL
response = requests.get(image_url)
img = Image.open(BytesIO(response.content))

# Menampilkan gambar
plt.figure(figsize=(8, 6))
plt.imshow(img)
plt.axis('off')  # Tidak menampilkan sumbu
plt.show()

import requests
from PIL import Image
import matplotlib.pyplot as plt
from io import BytesIO

# URL gambar di GitHub
image_url = "https://raw.githubusercontent.com/arknsa/image/main/on_18_3.jpg"

# Mengambil gambar dari URL
response = requests.get(image_url)
img = Image.open(BytesIO(response.content))

# Menampilkan gambar
plt.figure(figsize=(8, 6))
plt.imshow(img)
plt.axis('off')  # Tidak menampilkan sumbu
plt.show()

"""## After 18th February 2024"""

after_feb_18 = df[df['created_at'] > '2024-02-18 23:59:59+00:00']

after_feb_18.head()

# Find the top 3 most retweeted tweets after 18th
top_3_retweeted_after_18 = after_feb_18.nlargest(3, 'retweet_count')

# Find the top 3 most liked tweets after 18th
top_3_liked_after_18 = after_feb_18.nlargest(3, 'favorite_count')

# Find the top 3 most replied-to tweets after 18th
top_3_replied_after_18 = after_feb_18.nlargest(3, 'reply_count')

# Extract relevant information into DataFrames for each category
top_3_retweeted_after_18_info = pd.DataFrame({
    'Username': top_3_retweeted_after_18['username'],
    'Full Text': top_3_retweeted_after_18['full_text'],
    'Created At': top_3_retweeted_after_18['created_at'],
    'Retweet Count': top_3_retweeted_after_18['retweet_count']
})

top_3_liked_after_18_info = pd.DataFrame({
    'Username': top_3_liked_after_18['username'],
    'Full Text': top_3_liked_after_18['full_text'],
    'Created At': top_3_liked_after_18['created_at'],
    'Favorite Count': top_3_liked_after_18['favorite_count'],
    'Quote Count': top_3_liked_after_18['quote_count']
})

top_3_replied_after_18_info = pd.DataFrame({
    'Username': top_3_replied_after_18['username'],
    'Full Text': top_3_replied_after_18['full_text'],
    'Created At': top_3_replied_after_18['created_at'],
    'Reply Count': top_3_replied_after_18['reply_count']
})

top_3_retweeted_after_18_info

top_3_liked_after_18_info

top_3_replied_after_18_info

"""# Prepocessing Data

Peak = 18 Feb, apa yg bikin gitu? compare before after
"""

texts = df['full_text'].tolist()
texts

# Commented out IPython magic to ensure Python compatibility.
# %pip install Sastrawi

# Commented out IPython magic to ensure Python compatibility.
# %pip install nltk

import nltk
nltk.download('stopwords')

nltk.download('punkt')

import re
import string
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Combine stopwords from both languages (Indonesian and English)
stop_words = set(stopwords.words('indonesian') + stopwords.words('english'))

# Define a regular expression to match emojis
emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F700-\U0001F77F"  # alchemical symbols
                           # ... (remaining Unicode emoji blocks)
                           "]+", flags=re.UNICODE)

# Define a function to preprocess text
def preprocess_text(text):
    # Remove emojis
    text = emoji_pattern.sub(r'', text)
    # Remove URLs
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Convert to lowercase
    text = text.lower()

    custom_stopwords = ['sirekap']
    # Tokenize words and remove stop words and custom words
    words = [word for word in word_tokenize(text)
             if len(word) >= 4 and word not in stop_words and word not in custom_stopwords and not word.startswith('@') and not word.startswith('#')]
    return ' '.join(words)

# Preprocess all text in the dataset
preprocessed_texts = [preprocess_text(text) for text in texts]

# Initialize TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer()

# Transform text into TF-IDF matrix
tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_texts)

preprocessed_texts

texts2 = before_feb_18['full_text'].tolist()
texts2

# Preprocess all text in the dataset
preprocessed_texts2 = [preprocess_text(text) for text in texts2]

# Join the strings in the list into a single string
combined_text = ' '.join(preprocessed_texts2)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_text)

# Display the word cloud using matplotlib
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Remove axis labels
plt.title('Word Cloud Example')
plt.show()

texts3 = after_feb_18['full_text'].tolist()
texts3

# Preprocess all text in the dataset
preprocessed_texts3 = [preprocess_text(text) for text in texts3]

# Join the strings in the list into a single string
combined_text2= ' '.join(preprocessed_texts3)

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_text2)

# Display the word cloud using matplotlib
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Remove axis labels
plt.title('Word Cloud Example')
plt.show()

"""# TFIDF"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_matrix = tfidf_matrix.toarray()

import matplotlib.pyplot as plt
import pandas as pd

# Mendapatkan daftar kata-kata dari TF-IDF Vectorizer
feature_names = tfidf_vectorizer.get_feature_names_out()

# Menghitung nilai TF-IDF rata-rata untuk setiap kata
average_tfidf = tfidf_matrix.mean(axis=0).flatten()

# Membuat DataFrame untuk memudahkan pengurutan dan plotting
df2 = pd.DataFrame({'word': feature_names, 'tfidf': average_tfidf})

# Mengurutkan DataFrame berdasarkan nilai TF-IDF
df2 = df2.sort_values(by='tfidf', ascending=False)  # Change 'df' to 'df2'

# Mengambil 10 kata paling umum
top_10_words = df2.head(10)

# Plotting bar plot
plt.figure(figsize=(16, 9))
plt.barh(top_10_words['word'], top_10_words['tfidf'], color='skyblue')
plt.xlabel('TF-IDF Value')
plt.title('Top 10 Common Words After Preprocessing')
plt.show()

"""# Clustering Data"""

from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer
import matplotlib.pyplot as plt


from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()

# Define the K-Means model and K Elbow visualiser
model = KMeans(init='k-means++', random_state=42)  # Adjust random state if desired
visualizer = KElbowVisualizer(model, k=(2, 10))  # Adjust k range as needed

# Fit the data and visualize the elbow
visualizer.fit(tfidf_matrix)
visualizer.show()

"""# Word Cloud"""

clusters_labels = model.labels_

#clusters_labels = [1, 2, 3, 4, 5]

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Menginisialisasi WordCloud
wordcloud = WordCloud(width=1600, height=900, background_color='white')

# Loop untuk setiap kluster
for cluster in range(5):  # 4 kluster
    cluster_texts = [preprocessed_texts[i] for i, label in enumerate(clusters_labels) if label == cluster]
    cluster_texts = ' '.join(cluster_texts)

    # Membuat WordCloud untuk kluster
    wordcloud.generate(cluster_texts)

    # Menampilkan WordCloud
    plt.figure(figsize=(16, 9))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(f'Cluster {cluster}')
    plt.axis('off')
    plt.show()

"""# Social Network Analysis

"""

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from networkx.algorithms import community

import numpy as np
import networkx as nx

# Load the DataFrame
df_sna = pd.read_csv("integrated_sirekap_new.csv")
df_sna = df_sna[['created_at','id_str','full_text','username']]
df_sna.columns = ['Datetime', 'Tweet ID','Text', 'Username']
df_sna = df_sna.replace(np.nan, '')

num_tweets = len(df_sna)
print(f"Jumlah tweet dalam dataframe adalah {num_tweets}.")

display(df_sna.head())

# Create a directed graph
G = nx.DiGraph()

# Add edges (interactions) based on usernames in the DataFrame
for idx, row in df_sna.iterrows():
    tweet_id = row['Tweet ID']
    source_user = row['Username']
    text = row['Text']

    # Extract mentioned usernames from the tweet text
    mentioned_users = [word.strip('@') for word in text.split() if word.startswith('@')]

    # Add edges from source_user to mentioned_users
    for target_user in mentioned_users:
        if source_user != target_user:
            G.add_edge(source_user, target_user, tweet_id=tweet_id)

# Detect communities (clusters) in the network using the Louvain method
partition = community.greedy_modularity_communities(G)

# Calculate degree centrality for nodes
degree_centrality = nx.degree_centrality(G)

# Set threshold values for displaying nodes and edges
node_threshold = 0.05  # Display nodes with degree centrality >= threshold
edge_threshold = 2     # Display edges with weight (tweet count) >= threshold

# Filter nodes and edges based on thresholds
significant_nodes = [node for node, centrality in degree_centrality.items() if centrality >= node_threshold]
significant_edges = [(source, target) for source, target in G.edges() if G[source][target]['tweet_id'] >= edge_threshold]

# Create a subgraph with significant nodes and edges
significant_subgraph = G.subgraph(significant_nodes).copy()
significant_subgraph.add_edges_from(significant_edges)

# Draw the subgraph with node colors based on communities
plt.figure(figsize=(12, 8))
pos = nx.spring_layout(significant_subgraph, k=0.3, iterations=50)  # Adjust layout parameters if needed
node_colors = [i for i, comm in enumerate(partition) for node in comm if node in significant_subgraph.nodes]
nx.draw(significant_subgraph, pos, node_color=node_colors, cmap=plt.cm.tab20, with_labels=True, node_size=300, font_size=10, arrows=True)
plt.title("Social Network Analysis (SNA)")
plt.show()

# Identify the most influential users based on degree centrality in the subgraph
sorted_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)
print("Most Influential Users (Degree Centrality):")
for user, centrality in sorted_degree[:5]:
    print(f"{user}: {centrality:.3f}")

G_mention = nx.DiGraph()

for r in df_sna.iterrows():
  author = r[1]['Username']
  author = f'@{author}'

  text = r[1]['Text']

  try:
    timestamp = pd.to_datetime(r[1]['Datetime'])
  except:
    continue

  mentions = set(re.findall(r"@\w+", text))  # Updated regular expression for Twitter mentions


  if len(mentions)>0:
    for u in mentions:
      u = f'@{u}'
      G_mention.add_edge(author, u, Timestamp=timestamp)

import networkx as nx
from operator import itemgetter
# Define your specific type of graph, for example, a directed graph (DiGraph)
graph = nx.DiGraph()

# Add nodes to the graph
graph.add_node("A")
graph.add_node("B")
graph.add_node("C")

# Add edges to the graph
graph.add_edge("A", "B")
graph.add_edge("B", "C")

# Calculate Degree Centrality
graph_centrality = nx.degree_centrality(graph)
max_de = max(graph_centrality.items(), key=itemgetter(1))

# Calculate Closeness Centrality
graph_closeness = nx.closeness_centrality(graph)
max_clo = max(graph_closeness.items(), key=itemgetter(1))

# Calculate Betweenness Centrality
graph_betweenness = nx.betweenness_centrality(graph, normalized=True, endpoints=False)
max_bet = max(graph_betweenness.items(), key=itemgetter(1))

# Filter nodes based on centrality dictionary
filtered_nodes = [node for node in G_mention.nodes if node in graph_centrality]
pos = nx.spring_layout(G_mention)
# Calculate sizes and colors for filtered nodes
sizes = [graph_centrality[node] * 10000 for node in filtered_nodes]
colors = [graph_centrality[node] for node in filtered_nodes]

# Draw nodes, labels, and edges
nodes = nx.draw_networkx_nodes(G_mention, pos, alpha=0.8,
                               nodelist=filtered_nodes,  # Use filtered nodes
                               node_size=sizes, node_color=colors,
                               cmap=plt.cm.Blues)
nodes.set_edgecolor('k')
nx.draw_networkx_labels(G_mention, pos, font_size=8)
nx.draw_networkx_edges(G_mention, pos, width=1.0, alpha=0.2)
plt.show()

import networkx as nx
import pandas as pd
import re


# Step 1: Preprocess the text to extract mentions
def extract_mentions(text):
    # Use a regular expression to find mentions (usernames)
    mentions = re.findall(r'@(\w+)', text)
    return mentions

# Step 2: Create a mapping of mentions to user IDs
mention_to_user_id = {}
for index, row in df_sna.iterrows():
    user_id = row['Tweet ID']
    mentions = extract_mentions(row['Text'])
    for mention in mentions:
        mention_to_user_id[mention] = user_id

# Step 3: Use the mapping to establish connections (edges) between users
for index, row in df_sna.iterrows():
    user_id = row['Tweet ID']
    mentions = extract_mentions(row['Text'])
    for mention in mentions:
        if mention in mention_to_user_id:
            G.add_edge(user_id, mention_to_user_id[mention])

# Step 4: Create and visualize the graph
import matplotlib.pyplot as plt

pos = nx.spring_layout(G)
plt.figure(figsize=(10, 10))
nx.draw(G, pos, with_labels=True, node_size=10, font_size=8, node_color='skyblue', edge_color='gray', alpha=0.7)
plt.title('Twitter Mention Network')
plt.show()

df_mention = nx.to_pandas_edgelist(G_mention)
df_mention.to_csv('mention.csv', index=False)

# Load the tweet data from your CSV file with specified delimiter
df3 = pd.read_csv("mention.csv", delimiter=",")

# Display the DataFrame to view the separated columns
display(df3)

G = nx.from_pandas_edgelist(df3,'source','target')
pos = nx.spring_layout(G)


# Create a plot
f, ax = plt.subplots(figsize=(16, 9))
plt.style.use('ggplot')

# Draw nodes with an edgecolor
nodes = nx.draw_networkx_nodes(G, pos, alpha=0.8)
nodes.set_edgecolor('k')

# Draw labels and edges
nx.draw_networkx_labels(G, pos, font_size=10)  # Note the corrected function name
nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.2)
